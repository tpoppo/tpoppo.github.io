---
layout: distill
title: An overview of Gradient Boosting Tree
date: 2023-02-01
description: An overview of Gradient Boosting Tree
tags: ml
categories:

authors:
  - name: Lorenzo Rossi
    url: ../../../
    affiliations:
      name: ETH ZÃ¼rich

bibliography: 2023-02-01-gbt.bib

---
Gradient boosting tree is one of the most powerful and used algorithms for machine learning, in particular with tabular data.
Moreover, it is quite often used in the top Kaggle solutions.

Formally, we would like to solve the following task:
Given a loss function $$ L(y, \hat{y}) $$, we would like to find a function $$F(x)$$ that minimize the given loss in the data distribution $$D$$:
\begin{equation}
\underset{F}{\operatorname{argmin}} {\mathop{\mathbb{E}}_{(x, y) \sim D}{L(y, F(x))}}
\end{equation}

The main idea of the boosting method is to create $$F$$ as a weighted linear combination of weaker and simpler models $$h$$ (in the case of gradient boosting trees, they will be decision trees). Therefore:
\begin{equation}
F(x) = \sum_{i=1}^{m}{\beta_{i} h_{i}(x)} + c
\end{equation}

## Training 
Given a training set $${(x_1, y_1), ..., (x_n, y_n)}$$, this method tries to greedily pick the next best weak model $$h$$.
\begin{equation}
F_0(x) = \underset{c}{\operatorname{argmin}} \sum_{i=1}^{n}{L(y_i, c)}
\end{equation}
\begin{equation}
F_{m+1}(x) = F_m(x) + \underset{h_m}{\operatorname{argmin}} \sum_{i=1}^{n}{L(y_i, F_m(x_i)+h_m(x_i))}
\end{equation}

The second equation is generally intractable. Hence, we approximate this step using a gradient step, which could be the gradient descent or the Newton method.

\begin{equation}
\underset{h_m}{\operatorname{argmin}} \sum_{i=1}^{n}{L(y_i, F_m(x_i)+h_m(x_i))} \\
\end{equation}
\begin{equation}
\approx \underset{h_m}{\operatorname{argmin}} \sum_{i=1}^{n}{L(y_i, F_m(x_i)) + \nabla L(y_i, F_m(x_i)) h_m(x_i)} \\
\end{equation}
\begin{equation}
\approx \underset{h_m}{\operatorname{argmin}} \sum_{i=1}^{n}{\nabla L(y_i, F_m(x_i)) h_m(x_i)}
\end{equation}

Consequently, $$h_m(x)$$ should be close to $$-\nabla L(y_i, F_m(x_i))$$.

# Novel methods
In this section, we investigate the different implementations of gradient boosting trees and other novel improvements
### XGBoost: A Scalable Tree Boosting System <d-cite key="chen2016xgboost"></d-cite> 

Firstly, they used two widely used regularization methods which are:

1. Shrinkage scales, which means that the newly added decision tree $$h_m$$ is multiplied by a factor $$\gamma$$ smaller than 1, similar to gradient descent. This technique reduces the influence of each tree.
2. Column subsampling, which is a method used in Random Forest, reduces both overfitting and improves training performance.

One of the most important contributions of this work is an approximate algorithm to find the best split. While the basic exact algorithm would try $$O(NK)$$ splits with $$N$$ samples and $$K$$ features, the approximate algorithm tries to map the continuous features into buckets. To solve this mapping problem, the work proposed a new weighted quantile sketch with provable theoretical guarantees.

Another significant contribution of this work is the sparsity-aware split finding. In tabular datasets is quite common to have very sparse features, thus, when splitting the algorithm must decide also whether the missing values should go to the left or the right leaf. Therefore, the splitting algorithm should select (feature, bucket, default branch). This method allows us to speed up the training step when there are many missing values.

### LightGBM: A Highly Efficient Gradient Boosting Decision Tree <d-cite key="ke2017lightgbm"></d-cite> 
The main aim of LightGBM is to efficiently work when the feature dimension is high, and the data size is large.
Before going on the novelties, such as XGBoost, this method uses the histogram-based algorithm for splitting.

This work proposed two methods Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB).

The first one, Gradient-based One-Side Sampling, notices that samples with a large gradient are informative, while the smaller ones are less useful. However, only discarding small gradients is quite harmful. Thus, this method keeps all the large ones and performs random sampling on the small ones. Therefore, it first sorts by the absolute value of the gradients, then it picks the top $$a*100%$$ samples and an additional $$b*100%$$ of the instances randomly from the rest of the dataset. Finally, it amplifies the small gradients by a factor of $$\frac{1-a}{b}$$. Consequently, the more difficult samples are more important while not changing too much the data distribution.

The second method proposed is called Exclusive Feature Bundling, which aims to reduce the number of used features. The main idea of this approach is to group exclusive features into _exclusive feature bundles_, e.g. features that never take nonzero values simultaneously. However, it can be shown that the problem of partitioning features into the smallest number of exclusive bundles is NP-hard, as it is possible to reduce it to the graph coloring problem, which is known to be NP-hard. Thus, an approximate algorithm is used, additionally, it allows a small _conflict factor_ $$\gamma$$ for non-exclusive samples.
To merge a bundle of exclusive features, each feature is mapped to a given range e.g. feature A to [0, 10), features B to [10,20) and so on. In this way, it reduces the number of features in an almost lossless way.
